tireTread4DataFrame <- data.frame(tireTread4)
# Dummy code the Position (LF, RF, LR, RR) feature,
# convert it back into a tibble, and store the result into
# a new tireTread5 tibble
tireTread5 <- as_tibble(dummy.data.frame(data = tireTread4DataFrame,
names = "Position"))
View(tireTread5)
# DataCamp WebScrapping
install.packages("tidyverse")
load(tidyverse)
library(tidyverse)
# DataCamp WebScrapping
install.packages("tidyverse")
library(tidyverse)
html_excerpt_raw <- '
<html>
<body>
<h1>Web scraping is cool</h1>
<p>It involves writing code – be it R or Python.</p>
<p><a href="https://datacamp.com">DataCamp</a>
has courses on it.</p>
</body>
</html>'
# Turn the raw excerpt into an HTML document R understands
html_excerpt <- read_html(html_excerpt_raw)
install.packages("tidyverse")
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt)
html_excerpt_raw <- '
<html>
<body>
<h1>Web scraping is cool</h1>
<p>It involves writing code – be it R or Python.</p>
<p><a href="https://datacamp.com">DataCamp</a>
has courses on it.</p>
</body>
</html>'
# Turn the raw excerpt into an HTML document R understands
html_excerpt <- read_html(html_excerpt_raw)
library(rvest)
html_excerpt_raw <- '
<html>
<body>
<h1>Web scraping is cool</h1>
<p>It involves writing code – be it R or Python.</p>
<p><a href="https://datacamp.com">DataCamp</a>
has courses on it.</p>
</body>
</html>'
# Turn the raw excerpt into an HTML document R understands
html_excerpt <- read_html(html_excerpt_raw)
html_excerpt
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
??xml_structure
install.packages("XML")
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
library(XML)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
library(methods)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
getwd()
setwd("C:/Users/ual-laptop/Documents")
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
xml_excerpt <- read_xml(html_excerpt_raw)
xml_excerpt <- read.csv(html_excerpt_raw)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
library(xml2)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
# Print the HTML excerpt with the xml_structure() function
xml_structure("html_excerpt_raw")
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
install.packages(xml2)
install.packages("xml2")
install.packages("xml2")
library(xml2)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt_raw)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt)
# Print the HTML excerpt with the xml_structure() function
xml_structure("html_excerpt")
library(xml2)
html_excerpt_raw <- '
<html>
<body>
<h1>Web scraping is cool</h1>
<p>It involves writing code – be it R or Python.</p>
<p><a href="https://datacamp.com">DataCamp</a>
has courses on it.</p>
</body>
</html>'
# Turn the raw excerpt into an HTML document R understands
html_excerpt <- read_html(html_excerpt_raw)
# Print the HTML excerpt with the xml_structure() function
xml_structure(html_excerpt)
# Read in the corresponding HTML string
list_html <- read_html(list_raw_html)
list_html <- '
<html>
<body>
<ol>
<li>1. Learn HTML </li>
<li>2. Learn CSS </li>
<li>3. Learn R</li>
<li>4.Scrape everything!*</li>
</ol>
</body>
</html>
'
# Read in the corresponding HTML string
list_html <- read_html(list_raw_html)
list_html_raw <- '
<html>
<body>
<ol>
<li>1. Learn HTML </li>
<li>2. Learn CSS </li>
<li>3. Learn R</li>
<li>4.Scrape everything!*</li>
</ol>
</body>
</html>
'
# Read in the corresponding HTML string
list_html <- read_html(list_raw_html)
list_raw_html <- '
<html>
<body>
<ol>
<li>1. Learn HTML </li>
<li>2. Learn CSS </li>
<li>3. Learn R</li>
<li>4.Scrape everything!*</li>
</ol>
</body>
</html>
'
# Read in the corresponding HTML string
list_html <- read_html(list_raw_html)
# Extract the ol node
ol_node <- list_html %>% html_nodes('ol')
library(xml2)
library(tidyverse)
library(rvest)
list_raw_html <- '
<html>
<body>
<ol>
<li>1. Learn HTML </li>
<li>2. Learn CSS </li>
<li>3. Learn R</li>
<li>4.Scrape everything!*</li>
</ol>
</body>
</html>
'
# Read in the corresponding HTML string
list_html <- read_html(list_raw_html)
# Extract the ol node
ol_node <- list_html %>% html_nodes('ol')
hyperlink_raw_html <- '
<html>\n  <body>\n    <h3>Helpful links</h3>\n    <ul>\n      <li><a href=\"https://wikipedia.org\">Wikipedia</a></li>\n      <li><a href=\"https://dictionary.com\">Dictionary</a></li>\n      <li><a href=\"https://duckduckgo.com\">Search Engine</a></li>\n    </ul>\n    <small>\n      Compiled with help from <a href=\"https://google.com\">Google</a>.\n    </small>\n  </body>\n</html>
'
# Extract all the a nodes from the bulleted list
links <- hyperlink_raw_html %>%
read_html() %>%
html_nodes('li a') # 'ul a' is also correct!
# Extract the needed values for the data frame
domain_value = links %>% html_attr('href')
name_value = links  %>% html_text()
domain_value
name_value
# Construct a data frame
link_df <- tibble(
domain = domain_value,
name = name_value
)
link_df
library("tidyverse")
gps <- read_csv(file= "googleplaystore.csv",
col_types = "ccnnccciccDcc",
col_names = TRUE)
gps <- read_csv(file= "googleplaystore.csv",
col_types = "ccnnccciccDcc",
col_names = TRUE)
View(gps)
gps <- read_csv(file= "googleplaystore.csv",
col_types = "ccnnccciccccc",
col_names = TRUE)
summary(gps)
gps <- read_csv(file= "googleplaystore.csv",
col_types = "ccnnccciccccc",
col_names = TRUE)
summary(gps)
gps <- read_csv(file= "googleplaystore.csv",
col_types = "ccnnciciccccc",
col_names = TRUE)
summary(gps)
summary(gps$Rating)
gps <- read_csv(file= "googleplaystore.csv", na.string= c("NaN","NA",""))
library("tidyverse")
gpdata <- read_csv(file= "googleplaystore.csv", na.string= c("NaN","NA",""))
gpdata <- read_csv(file= "googleplaystore.csv", na.strings= c("NaN","NA",""))
gpdata <- read.csv(file= "googleplaystore.csv", na.strings= c("NaN","NA",""))
head(gpdata)
dim(gpdata)
gpdata <- gpdata[,-c(11,12,13)]
gpdata <- unique(gpdata)
dim(gpdata)
gpdata$App = as.character(gpdata$App)
gpdata$Category = as.character(gpdata$Category)
gpdata$Type = as.character(gpdata$Type)
gpdata$Content.Rating = as.character(gpdata$Content.Rating)
gpdata$Genres = as.character(gpdata$Genres)
gpdata$Rating = as.numeric(gpdata$Rating)
gpdata$Reviews <- as.numeric(gpdata$Reviews)
str(gpdata)
ind <- which(gpdata$Rating > 5)
gpdata <- gpdata[-ind,]
summary(gpdata$Rating)
gpdata$Size <-  str_replace_all(gpdata$Size, "M", "")
gpdata$Size <- ifelse(grepl( "k", gpdata$Size), as.numeric(gpdata$Size)/1000, gpdata$Size)
str(gpdata$Size)
View(gpdata)
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\+","")
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\,","")
gpdata$Installs <- as.integer(gpdata$Installs)
gpdata$Type = as.character(gpdata$Type)
gpdata$Installs <- as.integer(gpdata$Installs)
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\+","")
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\,","")
missing_values = gpdata %>%
map_df(function(i) sum(is.na(i))) %>%
gather(feature, total_null_count) %>%
arrange(desc(total_null_count))
gpdata <- na.omit(gpdata)
sum(is.na(gpdata))
glimpse(gpdata)
summary(gpdata$Reviews)
summary(gpdata$Installs)
gpdata$Reviews <- as.numeric(gpdata$Installs)
summary(gpdata$Installs)
gpdata$Reviews <- as.numeric(gpdata$Reviews)
gpdata$Installs <- as.numeric(gpdata$Installs)
library("tidyverse")
gpdata <- read.csv(file= "googleplaystore.csv", na.strings= c("NaN","NA",""))
head(gpdata)
dim(gpdata)
gpdata <- gpdata[,-c(11,12,13)]
gpdata <- unique(gpdata)
dim(gpdata)
gpdata$App = as.character(gpdata$App)
gpdata$Category = as.character(gpdata$Category)
gpdata$Type = as.character(gpdata$Type)
gpdata$Content.Rating = as.character(gpdata$Content.Rating)
gpdata$Genres = as.character(gpdata$Genres)
gpdata$Rating = as.numeric(gpdata$Rating)
gpdata$Reviews <- as.numeric(gpdata$Reviews)
str(gpdata)
ind <- which(gpdata$Rating > 5)
gpdata <- gpdata[-ind,]
summary(gpdata$Rating)
gpdata$Size <-  str_replace_all(gpdata$Size, "M", "")
gpdata$Size <- ifelse(grepl( "k", gpdata$Size), as.numeric(gpdata$Size)/1000, gpdata$Size)
str(gpdata$Size)
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\+","")
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\,","")
summary(gpdata$Installs)
str(gpdata$Installs)
gpdata$Installs <- as.factor(gpdata$Installs)
summary(gpdata$Installs)
gpdata$Installs <- as.integer(gpdata$Installs)
summary(gpdata$Installs)
gpdata$Installs <- as.numeric(gpdata$Installs)
summary(gpdata$Installs)
gpdata <- read.csv(file= "googleplaystore.csv", na.strings= c("NaN","NA",""))
head(gpdata)
dim(gpdata)
gpdata <- gpdata[,-c(11,12,13)]
gpdata <- unique(gpdata)
dim(gpdata)
gpdata$App = as.character(gpdata$App)
gpdata$Category = as.character(gpdata$Category)
gpdata$Type = as.character(gpdata$Type)
gpdata$Content.Rating = as.character(gpdata$Content.Rating)
gpdata$Genres = as.character(gpdata$Genres)
gpdata$Rating = as.numeric(gpdata$Rating)
gpdata$Reviews <- as.numeric(gpdata$Reviews)
str(gpdata)
ind <- which(gpdata$Rating > 5)
gpdata <- gpdata[-ind,]
summary(gpdata$Rating)
gpdata$Size <-  str_replace_all(gpdata$Size, "M", "")
gpdata$Size <- ifelse(grepl( "k", gpdata$Size), as.numeric(gpdata$Size)/1000, gpdata$Size)
str(gpdata$Size)
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\+","")
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\,","")
gpdata$Installs <- as.numeric(gpdata$Installs)
gpdata$Installs <- as.factor(gpdata$Installs)
gpdata <- read.csv(file= "googleplaystore.csv", na.strings= c("NaN","NA",""))
head(gpdata)
dim(gpdata)
gpdata <- gpdata[,-c(11,12,13)]
gpdata <- unique(gpdata)
dim(gpdata)
gpdata$App = as.character(gpdata$App)
gpdata$Category = as.character(gpdata$Category)
gpdata$Type = as.character(gpdata$Type)
gpdata$Content.Rating = as.character(gpdata$Content.Rating)
gpdata$Genres = as.character(gpdata$Genres)
gpdata$Rating = as.numeric(gpdata$Rating)
gpdata$Reviews <- as.numeric(gpdata$Reviews)
str(gpdata)
ind <- which(gpdata$Rating > 5)
gpdata <- gpdata[-ind,]
summary(gpdata$Rating)
gpdata$Size <-  str_replace_all(gpdata$Size, "M", "")
gpdata$Size <- ifelse(grepl( "k", gpdata$Size), as.numeric(gpdata$Size)/1000, gpdata$Size)
str(gpdata$Size)
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\+","")
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\,","")
summary(gpdata$Installs)
gpdata$Installs <- as.integer(gpdata$Installs)
summary(gpdata$Installs)
gpdata$Installs <- as.numeric(gpdata$Installs)
gpdata$Installs <- as.factor(gpdata$Installs)
gpdata <- read.csv(file= "googleplaystore.csv", na.strings= c("NaN","NA",""))
head(gpdata)
dim(gpdata)
gpdata <- gpdata[,-c(11,12,13)]
gpdata <- unique(gpdata)
dim(gpdata)
gpdata$App = as.character(gpdata$App)
gpdata$Category = as.character(gpdata$Category)
gpdata$Type = as.character(gpdata$Type)
gpdata$Content.Rating = as.character(gpdata$Content.Rating)
gpdata$Genres = as.character(gpdata$Genres)
gpdata$Rating = as.numeric(gpdata$Rating)
gpdata$Reviews <- as.numeric(gpdata$Reviews)
str(gpdata)
ind <- which(gpdata$Rating > 5)
gpdata <- gpdata[-ind,]
summary(gpdata$Rating)
gpdata$Size <-  str_replace_all(gpdata$Size, "M", "")
gpdata$Size <- ifelse(grepl( "k", gpdata$Size), as.numeric(gpdata$Size)/1000, gpdata$Size)
str(gpdata$Size)
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\+","")
gpdata$Installs <- str_replace_all(gpdata$Installs,"\\,","")
gpdata$Installs <- as.factor(gpdata$Installs)
gpdata$Installs <- as.integer(gpdata$Installs)
missing_values = gpdata %>%
map_df(function(i) sum(is.na(i))) %>%
gather(feature, total_null_count) %>%
arrange(desc(total_null_count))
gpdata <- na.omit(gpdata)
sum(is.na(gpdata))
glimpse(gpdata)
summary(gpdata$Reviews)
summary(gpdata$Installs)
# Set working Directory
setwd("C:\\Users\\ual-laptop\\Documents\\GitHub\\Workers Productivity Project")
# Read file using read_csv
garmentProd <- read_csv(file= "garments_worker_productivity.csv",
col_types = "Dffffnnnninnnn",
col_names = TRUE)
library(dummies)
library(tidyverse)
library(corrplot)
library(olsrr)
library(class)
library(e1071)
library(rpart)
library(rpart.plot)
library(dplyr)
library(neuralnet)
# Set working Directory
setwd("C:\\Users\\ual-laptop\\Documents\\GitHub\\Workers Productivity Project")
# Read file using read_csv
garmentProd <- read_csv(file= "garments_worker_productivity.csv",
col_types = "Dffffnnnninnnn",
col_names = TRUE)
# Display the garmentProd tibble on console
print(garmentProd)
# Display the structure of tibble
str(garmentProd)
# Display the summary of tibble
summary(garmentProd)
# Boxplot of work in progress
boxplotWip <- ggplot(data = garmentProd,
aes(x = wip))
boxplotWip + geom_boxplot(color = "blue",
fill = "lightblue")
# Boxplot of over_time
boxplotOvertime <- ggplot(data = garmentProd,
aes(x = over_time))
boxplotOvertime + geom_boxplot(color = "blue",
fill = "lightblue")
# Remove the unwanted columns
garmentProd <- garmentProd %>%
select(-idle_time, -idle_men, -no_of_style_change, -date)
# Display the summary of tibble
summary(garmentProd)
# Determine outliers in the over_time feature
# Calculate outlier min and max
outlierMinOver_time <- quantile(garmentProd$over_time, .25)-
(IQR(garmentProd$over_time)*1.5)
outlierMaxOver_time <- quantile(garmentProd$over_time, .75)+
(IQR(garmentProd$over_time)*1.5)
# Remove outliers from the dataset
garmentProd <- garmentProd %>%
filter(over_time >= outlierMinOver_time & over_time <= outlierMaxOver_time)
# Determine outliers in the incentive feature
outlierMinIncentive <- quantile(garmentProd$incentive, .25)-
(IQR(garmentProd$incentive)*1.5)
outlierMaxIncentive <- quantile(garmentProd$incentive, .75)+
(IQR(garmentProd$incentive)*1.5)
# Remove outliers from the incentive
garmentProd <- garmentProd %>%
filter(incentive >= outlierMinIncentive & incentive <=outlierMaxIncentive)
garmentProd1 <- garmentProd %>%
mutate(wip = ifelse(is.na(wip),
mean(wip, na.rm = TRUE),wip))
# Display the summary of tibble
summary(garmentProd1)
# Normalise the wip, smv, over_time,incentive features
garmentProd2 <- garmentProd1 %>%
mutate(wip = round((wip - min(wip))/
(max(wip) - min(wip)),2)) %>%
mutate(smv = round((smv - min(smv))/
(max(smv) - min(smv)),2)) %>%
mutate(over_time = round((over_time - min(over_time))/
(max(over_time) - min(over_time)),2)) %>%
mutate(incentive = round((incentive - min(incentive))/
(max(incentive) - min(incentive)),2))
# Display the summary of tibble
summary(garmentProd2)
# Display all Histograms
displayAllHistograms <- function(tibbleDataset){
tibbleDataset %>%
keep(is.numeric) %>%
gather() %>%
ggplot() + geom_histogram(mapping =aes(x=value, fill=key),
color="black")+
facet_wrap(~ key, scales="free") +
theme_minimal()
}
displayAllHistograms(garmentProd2)
# Analysing data using dplyr functions
# 1. Checking proportion of productivity vs unproductivity according to quarter
print(garmentProd2) %>%
group_by(quarter)%>%
summarize(
percentProductive =
length(actual_productivity[actual_productivity==TRUE])*100/
length(actual_productivity)
,
percentUnproductive =
length(actual_productivity[actual_productivity==FALSE])*100/
length(actual_productivity)
)
# 2. Checking proportion of productivity vs unproductivity according to
# day of week
print(garmentProd2 %>%
mutate(orderOfDay = case_when(
day =="Monday" ~ '1_Monday',
day =="Tuesday" ~ '2_Tuesday',
day =="Wednesday" ~ '3_Wednesday',
day =="Thursday" ~ '4_Thursday',
day =="Friday" ~ '5_Friday',
day =="Saturday" ~ '6_Saturday',
day =="Sunday" ~ '7_Sunday'
))) %>%
group_by(orderOfDay)%>%
summarize(
percentProductive =
length(actual_productivity[actual_productivity==TRUE])*100/
length(actual_productivity),
percentUnproductive =
length(actual_productivity[actual_productivity==FALSE])*100/
length(actual_productivity))
# 3. Checking productivity vs unproductivity according to number of workers
print(garmentProd2 %>%
mutate(workerCount = case_when(
no_of_workers <= 9 ~ '1_(0-9)',
no_of_workers <= 34 ~ '2_(10-34)',
no_of_workers <= 57 ~ '3_(35-57)',
no_of_workers >57 ~ '4_(>57)',
))) %>%
group_by(workerCount)%>%
summarize(
percentProductive =
length(actual_productivity[actual_productivity==TRUE])*100/
length(actual_productivity),
percentUnproductive =
length(actual_productivity[actual_productivity==FALSE])*100/
length(actual_productivity)
)
setwd("~")
setwd("C:\\Users\\ual-laptop\\Documents\\GitHub\\GooglePlayStoreApplicationProject")
install.packages("dummies",repos=NULL,type="source")
